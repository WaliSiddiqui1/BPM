{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/NSVA_Results/nsva_dataset.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/NSVA_Results')\n",
    "\n",
    "from nsva_model import NSVAModel, PositionalEncoding, DecoderLayer\n",
    "\n",
    "class NSVADataset:\n",
    "\n",
    "    def __init__(self, annotations_file, feature_paths, tokenizer, max_seq_length=30):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        try:\n",
    "            with open(annotations_file, 'r') as f:\n",
    "                self.annotations = json.load(f)\n",
    "            print(f\"Loaded annotations from {annotations_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading annotations: {e}\")\n",
    "            self.annotations = {'sentences': []}\n",
    "\n",
    "        self.feature_paths = feature_paths\n",
    "\n",
    "        self.video_id_to_caption = {}\n",
    "        for ann in self.annotations.get('sentences', []):\n",
    "            video_id = ann.get('video_id')\n",
    "            caption = ann.get('caption')\n",
    "            if video_id and caption:\n",
    "                self.video_id_to_caption[video_id] = caption\n",
    "\n",
    "        self.available_videos = []\n",
    "        for video_id in self.video_id_to_caption.keys():\n",
    "            if self.check_features_exist(video_id):\n",
    "                self.available_videos.append(video_id)\n",
    "\n",
    "    def check_features_exist(self, video_id):\n",
    "        for feature_type in self.feature_paths.keys():\n",
    "            feature_path = os.path.join(self.feature_paths[feature_type], f\"{video_id}.npy\")\n",
    "            if not os.path.exists(feature_path):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.available_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset with {len(self)} items\")\n",
    "\n",
    "        video_id = self.available_videos[idx]\n",
    "        caption = self.video_id_to_caption[video_id]\n",
    "\n",
    "        \n",
    "        features = {}\n",
    "        masks = {}\n",
    "\n",
    "        for feature_type in self.feature_paths.keys():\n",
    "            feature_path = os.path.join(self.feature_paths[feature_type], f\"{video_id}.npy\")\n",
    "            try:\n",
    "                feature = np.load(feature_path)\n",
    "\n",
    "                \n",
    "                mask = np.ones(feature.shape[0], dtype=np.int32)\n",
    "\n",
    "                features[feature_type] = feature\n",
    "                masks[feature_type] = mask\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {feature_type} feature for {video_id}: {e}\")\n",
    "                \n",
    "                features[feature_type] = np.zeros((1, 768), dtype=np.float32)  \n",
    "                masks[feature_type] = np.zeros(1, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"][0]\n",
    "        attention_mask = tokenized[\"attention_mask\"][0]\n",
    "\n",
    "        \n",
    "        decoder_input = tf.concat([[self.tokenizer.cls_token_id], input_ids[:-1]], axis=0)\n",
    "        target = input_ids\n",
    "\n",
    "        return {\n",
    "            'timesformer': (features['timesformer'], masks['timesformer']),\n",
    "            'ball': (features['ball'], masks['ball']),\n",
    "            'player': (features['player'], masks['player']),\n",
    "            'basket': (features['basket'], masks['basket']),\n",
    "            'court': (features['court'], masks['court']),\n",
    "            'target_ids': decoder_input,\n",
    "            'target_mask': attention_mask\n",
    "        }, target\n",
    "\n",
    "    def create_tf_dataset(self, batch_size=64, shuffle=True):\n",
    "        def generator():\n",
    "            import random\n",
    "            indices = list(range(len(self)))\n",
    "            if shuffle:\n",
    "                random.shuffle(indices)\n",
    "\n",
    "            for idx in indices:\n",
    "                try:\n",
    "                    yield self[idx]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating item {idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        output_shapes = (\n",
    "            {\n",
    "                'timesformer': (tf.TensorShape([None, None]), tf.TensorShape([None])),\n",
    "                'ball': (tf.TensorShape([None, None]), tf.TensorShape([None])),\n",
    "                'player': (tf.TensorShape([None, None, None]), tf.TensorShape([None])),\n",
    "                'basket': (tf.TensorShape([None, None]), tf.TensorShape([None])),\n",
    "                'court': (tf.TensorShape([None, None]), tf.TensorShape([None])),\n",
    "                'target_ids': tf.TensorShape([self.max_seq_length]),\n",
    "                'target_mask': tf.TensorShape([self.max_seq_length])\n",
    "            },\n",
    "            tf.TensorShape([self.max_seq_length])\n",
    "        )\n",
    "\n",
    "        \n",
    "        output_types = (\n",
    "            {\n",
    "                'timesformer': (tf.float32, tf.int32),\n",
    "                'ball': (tf.float32, tf.int32),\n",
    "                'player': (tf.float32, tf.int32),\n",
    "                'basket': (tf.float32, tf.int32),\n",
    "                'court': (tf.float32, tf.int32),\n",
    "                'target_ids': tf.int32,\n",
    "                'target_mask': tf.int32\n",
    "            },\n",
    "            tf.int32\n",
    "        )\n",
    "\n",
    "        \n",
    "        try:\n",
    "            dataset = tf.data.Dataset.from_generator(\n",
    "                generator,\n",
    "                output_types=output_types,\n",
    "                output_shapes=output_shapes\n",
    "            )\n",
    "\n",
    "            \n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating TensorFlow dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Dataset module loaded\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
