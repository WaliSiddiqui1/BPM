{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaliSiddiqui1/BPM/blob/main/Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dtbY-C6pikj1",
        "outputId": "ae980302-76e5-4102-88b5-acf3110f0ecd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import re\n",
        "import time\n",
        "\n",
        "!pip install -q webvtt-py\n",
        "!pip install -q transformers\n",
        "!pip install -q rouge\n",
        "!pip install -q nltk\n",
        "!pip install -q yolov5\n",
        "\n",
        "!pip install -q huggingface-hub>=0.30.0 --upgrade\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import webvtt\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/NSVA_Results/'\n",
        "RAW_VIDEOS_DIR = os.path.join(BASE_DIR, 'raw_videos')\n",
        "VTT_CAPTIONS_DIR = os.path.join(BASE_DIR, 'video_captions')\n",
        "FEATURES_DIR = os.path.join(BASE_DIR, 'features')\n",
        "ANNOTATIONS_DIR = os.path.join(BASE_DIR, 'annotations')\n",
        "CHECKPOINTS_DIR = os.path.join(BASE_DIR, 'checkpoints')\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, 'results')\n",
        "\n",
        "for directory in [RAW_VIDEOS_DIR, VTT_CAPTIONS_DIR, FEATURES_DIR, ANNOTATIONS_DIR,\n",
        "                 CHECKPOINTS_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "for feature_type in ['timesformer', 'ball', 'player', 'basket', 'court']:\n",
        "    os.makedirs(os.path.join(FEATURES_DIR, feature_type), exist_ok=True)\n",
        "\n",
        "METADATA_DIR = os.path.join(BASE_DIR, 'metadata')\n",
        "os.makedirs(METADATA_DIR, exist_ok=True)\n",
        "\n",
        "if os.path.exists(RAW_VIDEOS_DIR):\n",
        "    video_files = [f for f in os.listdir(RAW_VIDEOS_DIR) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
        "    print(f\"Found {len(video_files)} video files\")\n",
        "\n",
        "\n",
        "if os.path.exists(VTT_CAPTIONS_DIR):\n",
        "    vtt_files = [f for f in os.listdir(VTT_CAPTIONS_DIR) if f.endswith('.en.vtt')]\n",
        "    print(f\"Found {len(vtt_files)} VTT files\")\n",
        "\n",
        "def create_file_mapping():\n",
        "    vtt_files = [f for f in os.listdir(VTT_CAPTIONS_DIR) if f.endswith('.en.vtt')]\n",
        "    video_files = [f for f in os.listdir(RAW_VIDEOS_DIR) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
        "\n",
        "    mapping = {}\n",
        "\n",
        "    for vtt_file in vtt_files:\n",
        "        vtt_base = vtt_file[:-7]\n",
        "\n",
        "        for video_file in video_files:\n",
        "            video_base = os.path.splitext(video_file)[0]\n",
        "\n",
        "            if vtt_base == video_base:\n",
        "                mapping[vtt_file] = video_file\n",
        "                break\n",
        "\n",
        "    unmatched_vtts = [vtt for vtt in vtt_files if vtt not in mapping]\n",
        "    if unmatched_vtts:\n",
        "\n",
        "        for vtt_file in unmatched_vtts:\n",
        "            vtt_base = vtt_file[:-7]\n",
        "\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            for video_file in video_files:\n",
        "                video_base = os.path.splitext(video_file)[0]\n",
        "\n",
        "                common_chars = sum(1 for c in vtt_base if c in video_base)\n",
        "                score = common_chars / max(len(vtt_base), len(video_base))\n",
        "\n",
        "                if score > best_score and score > 0.7:\n",
        "                    best_match = video_file\n",
        "                    best_score = score\n",
        "\n",
        "            if best_match:\n",
        "                mapping[vtt_file] = best_match\n",
        "                print(f\"Matched '{vtt_file}' to '{best_match}'\")\n",
        "\n",
        "    with open(os.path.join(METADATA_DIR, 'file_mapping.txt'), 'w') as f:\n",
        "        for vtt, video in mapping.items():\n",
        "            f.write(f\"{vtt}|{video}\\n\")\n",
        "\n",
        "    return mapping\n",
        "\n",
        "file_mapping = create_file_mapping()\n",
        "\n",
        "def process_vtt_files(file_mapping):\n",
        "    annotations = {'sentences': []}\n",
        "    video_captions = {}\n",
        "\n",
        "    for vtt_file, video_file in tqdm(file_mapping.items()):\n",
        "        video_id = os.path.splitext(video_file)[0]\n",
        "        vtt_path = os.path.join(VTT_CAPTIONS_DIR, vtt_file)\n",
        "\n",
        "        try:\n",
        "            captions = webvtt.read(vtt_path)\n",
        "\n",
        "            video_captions[video_id] = []\n",
        "\n",
        "            for caption in captions:\n",
        "                start_time = caption.start\n",
        "                end_time = caption.end\n",
        "                text = caption.text.strip()\n",
        "\n",
        "                start_seconds = convert_time_to_seconds(start_time)\n",
        "                end_seconds = convert_time_to_seconds(end_time)\n",
        "\n",
        "                caption_entry = {\n",
        "                    'video_id': video_id,\n",
        "                    'start_time': start_time,\n",
        "                    'end_time': end_time,\n",
        "                    'start_seconds': start_seconds,\n",
        "                    'end_seconds': end_seconds,\n",
        "                    'caption': text\n",
        "                }\n",
        "\n",
        "                video_captions[video_id].append(caption_entry)\n",
        "\n",
        "                annotations['sentences'].append({\n",
        "                    'video_id': video_id,\n",
        "                    'caption': text,\n",
        "                    'start': start_seconds,\n",
        "                    'end': end_seconds\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {vtt_file}: {str(e)}\")\n",
        "\n",
        "    annotations_file = os.path.join(ANNOTATIONS_DIR, 'annotations.json')\n",
        "    with open(annotations_file, 'w') as f:\n",
        "        json.dump(annotations, f, indent=2)\n",
        "\n",
        "    for video_id, captions in video_captions.items():\n",
        "        video_captions_file = os.path.join(ANNOTATIONS_DIR, f'{video_id}_captions.json')\n",
        "        with open(video_captions_file, 'w') as f:\n",
        "            json.dump(captions, f, indent=2)\n",
        "    return annotations\n",
        "\n",
        "def convert_time_to_seconds(time_str):\n",
        "    h, m, s = time_str.split(':')\n",
        "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
        "\n",
        "video_files = [f for f in os.listdir(RAW_VIDEOS_DIR) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
        "\n",
        "def create_dataset_splits(annotations, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, min_test=35):\n",
        "    video_ids = list(set([s['video_id'] for s in annotations['sentences']]))\n",
        "\n",
        "    video_metadata = {}\n",
        "\n",
        "    for video_id in video_ids:\n",
        "        video_captions = [s for s in annotations['sentences'] if s['video_id'] == video_id]\n",
        "\n",
        "        team_mentions = []\n",
        "        for caption in video_captions:\n",
        "            team_pattern = r'\\b(Bucks|Lakers|Celtics|Warriors|Heat|Spurs|Bulls|Rockets)\\b'\n",
        "            teams_found = re.findall(team_pattern, caption['caption'])\n",
        "            team_mentions.extend(teams_found)\n",
        "\n",
        "        actions = {\n",
        "            'shot': sum(1 for c in video_captions if 'shot' in c['caption'].lower()),\n",
        "            'rebound': sum(1 for c in video_captions if 'rebound' in c['caption'].lower()),\n",
        "            'assist': sum(1 for c in video_captions if 'assist' in c['caption'].lower()),\n",
        "            'block': sum(1 for c in video_captions if 'block' in c['caption'].lower()),\n",
        "            'steal': sum(1 for c in video_captions if 'steal' in c['caption'].lower())\n",
        "        }\n",
        "\n",
        "        video_metadata[video_id] = {\n",
        "            'teams': list(set(team_mentions)),\n",
        "            'actions': actions,\n",
        "            'caption_count': len(video_captions)\n",
        "        }\n",
        "\n",
        "    import random\n",
        "    random.seed(42)\n",
        "\n",
        "    shuffled_videos = video_ids.copy()\n",
        "    random.shuffle(shuffled_videos)\n",
        "\n",
        "    n_videos = len(shuffled_videos)\n",
        "    test_size = max(int(n_videos * test_ratio), min_test)\n",
        "\n",
        "    remaining = n_videos - test_size\n",
        "\n",
        "    n_train = int(remaining * (train_ratio / (train_ratio + val_ratio)))\n",
        "    n_val = remaining - n_train\n",
        "\n",
        "    train_videos = shuffled_videos[:n_train]\n",
        "    val_videos = shuffled_videos[n_train:n_train+n_val]\n",
        "    test_videos = shuffled_videos[n_train+n_val:]\n",
        "\n",
        "    splits = {\n",
        "        'train': train_videos,\n",
        "        'val': val_videos,\n",
        "        'test': test_videos\n",
        "    }\n",
        "\n",
        "    splits_file = os.path.join(METADATA_DIR, 'splits.json')\n",
        "    with open(splits_file, 'w') as f:\n",
        "        json.dump(splits, f, indent=2)\n",
        "\n",
        "    print(f\"Created dataset splits: {len(train_videos)} train, {len(val_videos)} val, {len(test_videos)} test\")\n",
        "    return splits\n",
        "\n",
        "def resume_processing(file_mapping, splits):\n",
        "    processed_videos = set()\n",
        "    state_file = os.path.join(METADATA_DIR, 'processed_videos.txt')\n",
        "    if os.path.exists(state_file):\n",
        "        with open(state_file, 'r') as f:\n",
        "            processed_videos = set(f.read().splitlines())\n",
        "\n",
        "    unprocessed = {\n",
        "        'train': [v for v in splits['train'] if v not in processed_videos],\n",
        "        'val': [v for v in splits['val'] if v not in processed_videos],\n",
        "        'test': [v for v in splits['test'] if v not in processed_videos]\n",
        "    }\n",
        "\n",
        "    quotas = {\n",
        "        'train': min(5, len(unprocessed['train'])),\n",
        "        'val': min(5, len(unprocessed['val'])),\n",
        "        'test': min(30, len(unprocessed['test']))\n",
        "    }\n",
        "\n",
        "    to_process = {\n",
        "        'train': unprocessed['train'][:quotas['train']],\n",
        "        'val': unprocessed['val'][:quotas['val']],\n",
        "        'test': unprocessed['test'][:quotas['test']]\n",
        "    }\n",
        "\n",
        "    return to_process, processed_videos\n",
        "\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "\n",
        "def load_yolo_model():\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "    model.classes = [0, 32]\n",
        "    return model\n",
        "\n",
        "def load_vit_model():\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "    model = ViTModel.from_pretrained('google/vit-base-patch16-224', add_pooling_layer=False)\n",
        "    return feature_extractor, model\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, yolo_model, vit_extractor, vit_model, file_mapping):\n",
        "        self.yolo = yolo_model\n",
        "        self.vit_extractor = vit_extractor\n",
        "        self.vit_model = vit_model\n",
        "        self.file_mapping = file_mapping\n",
        "\n",
        "        self.video_id_to_file = {}\n",
        "        for vtt_file, video_file in file_mapping.items():\n",
        "            video_id = os.path.splitext(video_file)[0]\n",
        "            self.video_id_to_file[video_id] = video_file\n",
        "\n",
        "        self.processed_videos = set()\n",
        "        state_file = os.path.join(METADATA_DIR, 'processed_videos.txt')\n",
        "        if os.path.exists(state_file):\n",
        "            with open(state_file, 'r') as f:\n",
        "                self.processed_videos = set(f.read().splitlines())\n",
        "\n",
        "    def extract_frames(self, video_path, sample_rate=8):\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        interval = max(1, int(fps / sample_rate))\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % interval == 0:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(frame)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def detect_objects(self, frame):\n",
        "        pil_image = Image.fromarray(frame)\n",
        "        results = self.yolo(pil_image)\n",
        "        return results\n",
        "\n",
        "    def extract_ball_features(self, frame, detections):\n",
        "        ball_detections = detections.xyxy[0][detections.xyxy[0][:, 5] == 32]\n",
        "\n",
        "        if len(ball_detections) == 0:\n",
        "            return np.zeros(768)\n",
        "\n",
        "        best_ball = ball_detections[torch.argmax(ball_detections[:, 4])]\n",
        "        x1, y1, x2, y2 = best_ball[:4].int().cpu().numpy()\n",
        "\n",
        "        h, w = frame.shape[:2]\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "        if x1 >= x2 or y1 >= y2:\n",
        "            return np.zeros(768)\n",
        "\n",
        "        ball_crop = frame[y1:y2, x1:x2]\n",
        "\n",
        "        ball_crop_pil = Image.fromarray(ball_crop)\n",
        "        inputs = self.vit_extractor(images=ball_crop_pil, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vit_model(**inputs)\n",
        "\n",
        "        ball_features = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return ball_features\n",
        "\n",
        "    def extract_player_features(self, frame, detections):\n",
        "        player_detections = detections.xyxy[0][detections.xyxy[0][:, 5] == 0]\n",
        "\n",
        "        max_players = 5\n",
        "        player_features = np.zeros((max_players, 768))\n",
        "\n",
        "        for i, player in enumerate(player_detections[:max_players]):\n",
        "            x1, y1, x2, y2 = player[:4].int().cpu().numpy()\n",
        "\n",
        "            h, w = frame.shape[:2]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "            if x1 >= x2 or y1 >= y2:\n",
        "                continue\n",
        "\n",
        "            player_crop = frame[y1:y2, x1:x2]\n",
        "\n",
        "            player_crop_pil = Image.fromarray(player_crop)\n",
        "            inputs = self.vit_extractor(images=player_crop_pil, return_tensors=\"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.vit_model(**inputs)\n",
        "\n",
        "            player_features[i] = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return player_features\n",
        "\n",
        "    def extract_basket_features(self, frame, detections):\n",
        "        h, w = frame.shape[:2]\n",
        "        upper_frame = frame[:h//3, :]\n",
        "\n",
        "        basket_crop_pil = Image.fromarray(upper_frame)\n",
        "        inputs = self.vit_extractor(images=basket_crop_pil, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vit_model(**inputs)\n",
        "\n",
        "        basket_features = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return basket_features\n",
        "\n",
        "    def generate_court_segmentation(self, frame):\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "        edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "        edges_pil = Image.fromarray(edges_rgb)\n",
        "\n",
        "        inputs = self.vit_extractor(images=edges_pil, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vit_model(**inputs)\n",
        "\n",
        "        court_features = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return court_features\n",
        "\n",
        "    def extract_features(self, video_id, max_frames=100):\n",
        "        if video_id in self.processed_videos:\n",
        "            print(f\"Video {video_id} already processed. Skipping.\")\n",
        "            return True\n",
        "\n",
        "        print(f\"Processing video {video_id}\")\n",
        "\n",
        "        video_file = self.video_id_to_file[video_id]\n",
        "        video_path = os.path.join(RAW_VIDEOS_DIR, video_file)\n",
        "\n",
        "        frames = self.extract_frames(video_path)\n",
        "        if len(frames) == 0:\n",
        "            print(f\"Failed to extract frames from {video_path}\")\n",
        "            return False\n",
        "\n",
        "        frames = frames[:max_frames]\n",
        "        n_frames = len(frames)\n",
        "\n",
        "        timesformer_features = np.zeros((n_frames, 768))\n",
        "        ball_features = np.zeros((n_frames, 768))\n",
        "        player_features = np.zeros((n_frames, 5, 768))\n",
        "        basket_features = np.zeros((n_frames, 768))\n",
        "        court_features = np.zeros((n_frames, 768))\n",
        "\n",
        "        for i, frame in enumerate(tqdm(frames)):\n",
        "            detections = self.detect_objects(frame)\n",
        "\n",
        "            ball_features[i] = self.extract_ball_features(frame, detections)\n",
        "            player_features[i] = self.extract_player_features(frame, detections)\n",
        "            basket_features[i] = self.extract_basket_features(frame, detections)\n",
        "            court_features[i] = self.generate_court_segmentation(frame)\n",
        "\n",
        "            timesformer_features[i] = np.random.randn(768) * 0.1\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                np.save(os.path.join(FEATURES_DIR, 'ball', f\"{video_id}_temp.npy\"), ball_features[:i+1])\n",
        "                np.save(os.path.join(FEATURES_DIR, 'player', f\"{video_id}_temp.npy\"), player_features[:i+1])\n",
        "                np.save(os.path.join(FEATURES_DIR, 'basket', f\"{video_id}_temp.npy\"), basket_features[:i+1])\n",
        "                np.save(os.path.join(FEATURES_DIR, 'court', f\"{video_id}_temp.npy\"), court_features[:i+1])\n",
        "\n",
        "        np.save(os.path.join(FEATURES_DIR, 'timesformer', f\"{video_id}.npy\"), timesformer_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'ball', f\"{video_id}.npy\"), ball_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'player', f\"{video_id}.npy\"), player_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'basket', f\"{video_id}.npy\"), basket_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'court', f\"{video_id}.npy\"), court_features)\n",
        "\n",
        "        for feature_type in ['ball', 'player', 'basket', 'court']:\n",
        "            temp_file = os.path.join(FEATURES_DIR, feature_type, f\"{video_id}_temp.npy\")\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "        self.processed_videos.add(video_id)\n",
        "        with open(os.path.join(METADATA_DIR, 'processed_videos.txt'), 'a') as f:\n",
        "            f.write(f\"{video_id}\\n\")\n",
        "\n",
        "        return True\n",
        "\n",
        "def create_captions_csv(annotations):\n",
        "\n",
        "    captions_data = []\n",
        "\n",
        "    for sentence in annotations['sentences']:\n",
        "        video_id = sentence['video_id']\n",
        "        caption = sentence['caption']\n",
        "\n",
        "        captions_data.append({\n",
        "            'video_id': video_id,\n",
        "            'caption': caption,\n",
        "            'feature_file': f\"{video_id}.npy\"\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(captions_data)\n",
        "    df.to_csv(os.path.join(METADATA_DIR, 'captions.csv'), index=False)\n",
        "    print(f\"Exported {len(captions_data)} captions to CSV\")\n",
        "\n",
        "def main():\n",
        "    file_mapping = create_file_mapping()\n",
        "\n",
        "    annotations = process_vtt_files(file_mapping)\n",
        "\n",
        "    splits = create_dataset_splits(annotations)\n",
        "\n",
        "    create_captions_csv(annotations)\n",
        "\n",
        "    to_process, processed_videos = resume_processing(file_mapping, splits)\n",
        "\n",
        "    yolo_model = load_yolo_model()\n",
        "    vit_extractor, vit_model = load_vit_model()\n",
        "\n",
        "    extractor = FeatureExtractor(yolo_model, vit_extractor, vit_model, file_mapping)\n",
        "    extractor.processed_videos = processed_videos\n",
        "\n",
        "    train_videos = to_process['train']\n",
        "\n",
        "    for i, video_id in enumerate(train_videos):\n",
        "        success = extractor.extract_features(video_id)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(\"Clearing memory cache\")\n",
        "            torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    val_videos = to_process['val']\n",
        "\n",
        "    for i, video_id in enumerate(val_videos):\n",
        "        success = extractor.extract_features(video_id)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(\"Clearing memory cache\")\n",
        "            torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    test_videos = to_process['test']\n",
        "\n",
        "    for i, video_id in enumerate(test_videos):\n",
        "        success = extractor.extract_features(video_id)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(\"Clearing memory cache\")\n",
        "            torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMHMEBEdFdvSwO/Sll3LkZD",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
