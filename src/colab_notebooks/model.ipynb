{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/NSVA_Results/nsva_model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, max_position):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(d_model)[np.newaxis, :],\n",
    "            d_model\n",
    "        )\n",
    "\n",
    "        \n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim//num_heads\n",
    "        )\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim//num_heads\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask=None, enc_padding_mask=None, training=False):\n",
    "        \n",
    "        attn1 = self.mha1(\n",
    "            query=x, key=x, value=x,\n",
    "            attention_mask=look_ahead_mask,\n",
    "            training=training\n",
    "        )\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        \n",
    "        attn2 = self.mha2(\n",
    "            query=out1, key=enc_output, value=enc_output,\n",
    "            attention_mask=enc_padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3\n",
    "\n",
    "class NSVAModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, max_caption_length=30, embed_dim=256, num_heads=4, dff=512):\n",
    "        super(NSVAModel, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "\n",
    "        \n",
    "        self.timesformer_projection = tf.keras.layers.Dense(embed_dim)\n",
    "        self.ball_projection = tf.keras.layers.Dense(embed_dim)\n",
    "        self.player_projection = tf.keras.layers.Dense(embed_dim)\n",
    "        self.basket_projection = tf.keras.layers.Dense(embed_dim)\n",
    "        self.court_projection = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        \n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        \n",
    "        self.decoder_pos_encoding = PositionalEncoding(embed_dim, max_caption_length)\n",
    "\n",
    "        \n",
    "        self.temporal_encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-6),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(embed_dim//2, return_sequences=True)\n",
    "            ),\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        self.cross_encoder_layers = 3\n",
    "        self.cross_attention_layers = []\n",
    "        self.cross_ffn_layers = []\n",
    "        self.cross_norm1_layers = []\n",
    "        self.cross_norm2_layers = []\n",
    "\n",
    "        for _ in range(self.cross_encoder_layers):\n",
    "            self.cross_attention_layers.append(\n",
    "                tf.keras.layers.MultiHeadAttention(\n",
    "                    num_heads=num_heads, key_dim=embed_dim//num_heads\n",
    "                )\n",
    "            )\n",
    "            self.cross_ffn_layers.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(embed_dim*4, activation='relu'),\n",
    "                tf.keras.layers.Dense(embed_dim)\n",
    "            ]))\n",
    "            self.cross_norm1_layers.append(tf.keras.layers.LayerNormalization(epsilon=1e-6))\n",
    "            self.cross_norm2_layers.append(tf.keras.layers.LayerNormalization(epsilon=1e-6))\n",
    "\n",
    "        \n",
    "        self.decoder_layers = []\n",
    "        for _ in range(3):  \n",
    "            self.decoder_layers.append(DecoderLayer(embed_dim, num_heads, embed_dim*4))\n",
    "\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def process_features(self, inputs, training=False):\n",
    "        \n",
    "        timesformer_features = inputs['timesformer'][0]\n",
    "        ball_features = inputs['ball'][0]\n",
    "        player_features = inputs['player'][0]\n",
    "        basket_features = inputs['basket'][0]\n",
    "        court_features = inputs['court'][0]\n",
    "\n",
    "        \n",
    "        \n",
    "        if len(timesformer_features.shape) == 2:\n",
    "            timesformer_features = tf.expand_dims(timesformer_features, axis=1)\n",
    "\n",
    "        \n",
    "        if len(ball_features.shape) == 2:\n",
    "            ball_features = tf.expand_dims(ball_features, axis=1)\n",
    "\n",
    "        \n",
    "        if len(player_features.shape) == 3:\n",
    "            \n",
    "            pass\n",
    "        elif len(player_features.shape) == 4:\n",
    "            \n",
    "            \n",
    "            player_features = tf.reduce_mean(player_features, axis=2)\n",
    "        else:\n",
    "            \n",
    "            player_features = tf.expand_dims(player_features, axis=1)\n",
    "\n",
    "        \n",
    "        if len(basket_features.shape) == 2:\n",
    "            basket_features = tf.expand_dims(basket_features, axis=1)\n",
    "\n",
    "        \n",
    "        if len(court_features.shape) == 2:\n",
    "            court_features = tf.expand_dims(court_features, axis=1)\n",
    "\n",
    "        \n",
    "        timesformer_embed = self.timesformer_projection(timesformer_features)\n",
    "        ball_embed = self.ball_projection(ball_features)\n",
    "        player_embed = self.player_projection(player_features)\n",
    "        basket_embed = self.basket_projection(basket_features)\n",
    "        court_embed = self.court_projection(court_features)\n",
    "\n",
    "        \n",
    "        \n",
    "        object_features = ball_embed + player_embed + basket_embed\n",
    "\n",
    "        \n",
    "        timesformer_encoded = self.temporal_encoder(timesformer_embed)\n",
    "        object_encoded = self.temporal_encoder(object_features)\n",
    "        court_encoded = self.temporal_encoder(court_embed)\n",
    "\n",
    "        \n",
    "        fine_grained_encoded = tf.concat([object_encoded, court_encoded], axis=1)\n",
    "        encoder_output = tf.concat([timesformer_encoded, fine_grained_encoded], axis=1)\n",
    "\n",
    "        \n",
    "        for i in range(self.cross_encoder_layers):\n",
    "            \n",
    "            attn_output = self.cross_attention_layers[i](\n",
    "                query=encoder_output,\n",
    "                key=encoder_output,\n",
    "                value=encoder_output,\n",
    "                training=training\n",
    "            )\n",
    "\n",
    "            \n",
    "            encoder_output = self.cross_norm1_layers[i](encoder_output + attn_output)\n",
    "\n",
    "            \n",
    "            ffn_output = self.cross_ffn_layers[i](encoder_output)\n",
    "\n",
    "            \n",
    "            encoder_output = self.cross_norm2_layers[i](encoder_output + ffn_output)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        encoder_output = self.process_features(inputs, training)\n",
    "\n",
    "        \n",
    "        if 'target_ids' not in inputs or inputs['target_ids'] is None:\n",
    "            return encoder_output\n",
    "\n",
    "        \n",
    "        target_ids = inputs['target_ids']\n",
    "\n",
    "        \n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(target_ids)[1])\n",
    "\n",
    "        \n",
    "        decoder_input = self.decoder_embedding(target_ids)\n",
    "        decoder_input = self.decoder_pos_encoding(decoder_input)\n",
    "\n",
    "        \n",
    "        decoder_output = decoder_input\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(\n",
    "                decoder_output,\n",
    "                encoder_output,\n",
    "                look_ahead_mask,\n",
    "                training=training\n",
    "            )\n",
    "\n",
    "        \n",
    "        logits = self.final_layer(decoder_output)\n",
    "        return logits\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        \n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "    def generate_caption(self, features, tokenizer, max_length=30):\n",
    "        try:\n",
    "            \n",
    "            encoder_output = self.process_features(features, training=False)\n",
    "            print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "\n",
    "            \n",
    "            start_token = tokenizer.cls_token_id\n",
    "            end_token = tokenizer.sep_token_id\n",
    "            print(f\"Start token: {start_token}, End token: {end_token}\")\n",
    "\n",
    "            \n",
    "            current_tokens = tf.constant([[start_token]], dtype=tf.int32)\n",
    "\n",
    "            \n",
    "            for step in range(max_length):\n",
    "                \n",
    "                decoder_input = self.decoder_embedding(current_tokens)\n",
    "                decoder_input = self.decoder_pos_encoding(decoder_input)\n",
    "\n",
    "                \n",
    "                seq_len = tf.shape(current_tokens)[1]\n",
    "                look_ahead_mask = self.create_look_ahead_mask(seq_len)\n",
    "\n",
    "                \n",
    "                decoder_output = decoder_input\n",
    "                for layer in self.decoder_layers:\n",
    "                    decoder_output = layer(\n",
    "                        decoder_output,\n",
    "                        encoder_output,\n",
    "                        look_ahead_mask,\n",
    "                        training=False\n",
    "                    )\n",
    "\n",
    "                \n",
    "                logits = self.final_layer(decoder_output)\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "\n",
    "                \n",
    "                top_values, top_indices = tf.math.top_k(next_token_logits[0], k=5)\n",
    "                print(f\"Step {step}, Top 5 tokens: {top_indices.numpy()}, Probs: {tf.nn.softmax(top_values).numpy()}\")\n",
    "\n",
    "                \n",
    "                next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "                next_token = tf.reshape(next_token, [1, 1])\n",
    "\n",
    "                \n",
    "                current_tokens = tf.concat([current_tokens, next_token], axis=1)\n",
    "\n",
    "                \n",
    "                next_token_val = int(next_token.numpy()[0][0])\n",
    "                print(f\"Generated token: {next_token_val}, Text so far: {tokenizer.decode(current_tokens[0].numpy())}\")\n",
    "\n",
    "                \n",
    "                if next_token_val == end_token:\n",
    "                    print(\"Generated end token, stopping.\")\n",
    "                    break\n",
    "\n",
    "            \n",
    "            with_special = tokenizer.decode(current_tokens[0].numpy())\n",
    "            without_special = tokenizer.decode(current_tokens[0].numpy(), skip_special_tokens=True)\n",
    "            print(f\"Final with special tokens: {with_special}\")\n",
    "            print(f\"Final without special tokens: {without_special}\")\n",
    "\n",
    "            return current_tokens\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error in generate_caption_simple: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "            return tf.constant([[start_token, end_token]], dtype=tf.int32)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Model module loaded\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
