{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q webvtt-py\n",
    "!pip install -q rouge\n",
    "!pip install -q nltk\n",
    "!pip install -q transformers\n",
    "!pip install -q seaborn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/NSVA_Results')\n",
    "\n",
    "\n",
    "from nsva_dataset import NSVADataset\n",
    "from nsva_model import NSVAModel, PositionalEncoding, DecoderLayer\n",
    "\n",
    "def caption_loss_function(real, pred, mask=None):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "    loss = loss_object(real, pred)\n",
    "\n",
    "    if mask is not None:\n",
    "        \n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        \n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, inputs, target, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = caption_loss_function(target, predictions, inputs['target_mask'])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs=20, lr=0.001, patience=3,\n",
    "                checkpoint_path='checkpoints/model'):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    \n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        \n",
    "        train_loss.reset_state()\n",
    "\n",
    "        \n",
    "        for step, (inputs, target) in enumerate(tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            batch_loss = train_step(model, inputs, target, optimizer)\n",
    "            train_loss.update_state(batch_loss)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Step {step}, Loss: {batch_loss:.4f}')\n",
    "\n",
    "        \n",
    "        history['loss'].append(float(train_loss.result()))\n",
    "\n",
    "        \n",
    "        if val_dataset:\n",
    "            val_loss.reset_state()\n",
    "\n",
    "            for inputs, target in tqdm(val_dataset, desc=\"Validation\"):\n",
    "                predictions = model(inputs, training=False)\n",
    "                v_loss = caption_loss_function(target, predictions, inputs['target_mask'])\n",
    "                val_loss.update_state(v_loss)\n",
    "\n",
    "            \n",
    "            history['val_loss'].append(float(val_loss.result()))\n",
    "\n",
    "            \n",
    "            if val_loss.result() < best_val_loss:\n",
    "                best_val_loss = val_loss.result()\n",
    "                model.save_weights(f'{checkpoint_path}_best.weights.h5')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                    break\n",
    "\n",
    "        \n",
    "        model.save_weights(f'{checkpoint_path}_{epoch+1}.weights.h5')\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {train_loss.result():.4f}, Val Loss: {val_loss.result():.4f}')\n",
    "        print(f'Time taken: {time.time() - start:.2f} secs\\n')\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_dataset, tokenizer, num_samples=None, output_path=None):\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    example_pairs = []\n",
    "\n",
    "    \n",
    "    rouge = Rouge()\n",
    "\n",
    "    \n",
    "    test_iterator = iter(test_dataset)\n",
    "\n",
    "    sample_count = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            if num_samples is not None and sample_count >= num_samples:\n",
    "                break\n",
    "\n",
    "            \n",
    "            try:\n",
    "                inputs, targets = next(test_iterator)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "            \n",
    "            for i in range(len(targets)):\n",
    "                if num_samples is not None and sample_count >= num_samples:\n",
    "                    break\n",
    "\n",
    "                sample_count += 1\n",
    "\n",
    "                \n",
    "                example_inputs = {\n",
    "                    'timesformer': (tf.expand_dims(inputs['timesformer'][0][i], 0),\n",
    "                                  tf.expand_dims(inputs['timesformer'][1][i], 0)),\n",
    "                    'ball': (tf.expand_dims(inputs['ball'][0][i], 0),\n",
    "                           tf.expand_dims(inputs['ball'][1][i], 0)),\n",
    "                    'player': (tf.expand_dims(inputs['player'][0][i], 0),\n",
    "                             tf.expand_dims(inputs['player'][1][i], 0)),\n",
    "                    'basket': (tf.expand_dims(inputs['basket'][0][i], 0),\n",
    "                             tf.expand_dims(inputs['basket'][1][i], 0)),\n",
    "                    'court': (tf.expand_dims(inputs['court'][0][i], 0),\n",
    "                            tf.expand_dims(inputs['court'][1][i], 0)),\n",
    "                }\n",
    "\n",
    "                \n",
    "                gt_caption = tokenizer.decode(targets[i].numpy(), skip_special_tokens=True)\n",
    "\n",
    "                \n",
    "                try:\n",
    "                    caption_ids = model.generate_caption(example_inputs, tokenizer)\n",
    "                    \n",
    "                    gen_caption = tokenizer.decode(caption_ids[0].numpy(), skip_special_tokens=False)\n",
    "                    print(f\"GT: {gt_caption}\")\n",
    "                    print(f\"Pred (with special): {gen_caption}\")\n",
    "                    print(f\"Pred (without special): {tokenizer.decode(caption_ids[0].numpy(), skip_special_tokens=True)}\")\n",
    "\n",
    "                    references.append([gt_caption.split()])\n",
    "                    hypotheses.append(gen_caption.split())\n",
    "                    example_pairs.append((gt_caption, gen_caption))\n",
    "\n",
    "                    if sample_count % 5 == 0:\n",
    "                        print(f\"\\nExample {sample_count}:\")\n",
    "                        print(f\"GT: {gt_caption}\")\n",
    "                        print(f\"Pred: {gen_caption}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating caption: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    \n",
    "    if references and hypotheses:\n",
    "        bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "        bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        results['bleu1'] = bleu1\n",
    "        results['bleu2'] = bleu2\n",
    "        results['bleu3'] = bleu3\n",
    "        results['bleu4'] = bleu4\n",
    "\n",
    "        print(f\"\\nBLEU-1: {bleu1:.4f}\")\n",
    "        print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "        print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "        print(f\"BLEU-4: {bleu4:.4f}\")\n",
    "\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            rouge_refs = [' '.join(ref[0]) for ref in references]\n",
    "            rouge_hyps = [' '.join(hyp) for hyp in hypotheses]\n",
    "\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(rouge_hyps, rouge_refs, avg=True)\n",
    "\n",
    "            results['rouge_1'] = rouge_scores['rouge-1']['f']\n",
    "            results['rouge_2'] = rouge_scores['rouge-2']['f']\n",
    "            results['rouge_l'] = rouge_scores['rouge-l']['f']\n",
    "\n",
    "            print(f\"ROUGE-1: {results['rouge_1']:.4f}\")\n",
    "            print(f\"ROUGE-2: {results['rouge_2']:.4f}\")\n",
    "            print(f\"ROUGE-L: {results['rouge_l']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE scores: {e}\")\n",
    "\n",
    "    \n",
    "    if output_path:\n",
    "        results['examples'] = example_pairs[:10]  \n",
    "\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "def visualize_training_history(history, output_dir):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss', marker='o')\n",
    "    if 'val_loss' in history:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss', marker='x')\n",
    "\n",
    "    plt.title('Training and Validation Loss', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_history.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_evaluation_metrics(results, output_dir):\n",
    "    \n",
    "    metric_names = []\n",
    "    metric_values = []\n",
    "\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        key = f'bleu{i}'\n",
    "        if key in results:\n",
    "            metric_names.append(f'BLEU-{i}')\n",
    "            metric_values.append(results[key])\n",
    "\n",
    "    \n",
    "    rouge_keys = [k for k in results if k.startswith('rouge_')]\n",
    "    for key in rouge_keys:\n",
    "        metric_names.append(f'ROUGE-{key[-1]}')\n",
    "        metric_values.append(results[key])\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(metric_names, metric_values, color=plt.cm.viridis(np.linspace(0, 0.8, len(metric_names))))\n",
    "\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height+0.01,\n",
    "                f'{height:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "    plt.title('Evaluation Metrics', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.ylim(0, max(metric_values) * 1.15)  \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'evaluation_metrics.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_caption_examples(example_pairs, output_dir, num_examples=5):\n",
    "    \n",
    "    examples = example_pairs[:num_examples]\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, num_examples * 1.2))\n",
    "    ax.axis('off')\n",
    "\n",
    "    \n",
    "    cell_text = []\n",
    "    for i, (gt, gen) in enumerate(examples):\n",
    "        cell_text.append([f\"Example {i+1}\", gt, gen])\n",
    "\n",
    "    table = ax.table(cellText=cell_text,\n",
    "                     colLabels=[\"\", \"Ground Truth\", \"Generated Caption\"],\n",
    "                     cellLoc='left',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.1, 0.45, 0.45])\n",
    "\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "\n",
    "    \n",
    "    for j in range(3):\n",
    "        table[(0, j)].set_text_props(color='white', fontweight='bold')\n",
    "\n",
    "    plt.title('Ground Truth vs. Generated Captions', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'caption_examples.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def create_results_dashboard(history, results, example_pairs, output_dir):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1.5])\n",
    "\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(history.get('loss', []), marker='o', label='Training Loss')\n",
    "    if 'val_loss' in history:\n",
    "        ax1.plot(history.get('val_loss', []), marker='x', label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    \n",
    "    metric_names = []\n",
    "    metric_values = []\n",
    "\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        key = f'bleu{i}'\n",
    "        if key in results:\n",
    "            metric_names.append(f'BLEU-{i}')\n",
    "            metric_values.append(results[key])\n",
    "\n",
    "    \n",
    "    rouge_keys = [k for k in results if k.startswith('rouge_')]\n",
    "    for key in rouge_keys:\n",
    "        metric_names.append(f'ROUGE-{key[-1]}')\n",
    "        metric_values.append(results[key])\n",
    "\n",
    "    bars = ax2.bar(metric_names, metric_values, color=plt.cm.viridis(np.linspace(0, 0.8, len(metric_names))))\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height+0.01,\n",
    "                f'{height:.4f}', ha='center', fontsize=9)\n",
    "\n",
    "    ax2.set_title('Evaluation Metrics')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    feature_types = ['TimeSformer', 'Ball', 'Player', 'Basket', 'Court']\n",
    "    importance = [0.45, 0.15, 0.20, 0.10, 0.10]  \n",
    "\n",
    "    bars = ax3.bar(feature_types, importance, color=plt.cm.plasma(np.linspace(0.2, 0.8, len(feature_types))))\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height+0.01,\n",
    "                f'{height:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "    ax3.set_title('Approximate Feature Importance')\n",
    "    ax3.set_ylabel('Relative Importance')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    ax4.text(0.5, 0.5, \"NSVA Feature Processing Pipeline\",\n",
    "            ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax4.text(0.5, 0.3, \"TimeSformer → Object Detection → Court Analysis → Caption\",\n",
    "            ha='center', va='center', fontsize=12)\n",
    "    ax4.axis('off')\n",
    "\n",
    "    \n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    ax5.axis('off')\n",
    "\n",
    "    \n",
    "    examples = example_pairs[:3]\n",
    "\n",
    "    \n",
    "    cell_text = []\n",
    "    for i, (gt, gen) in enumerate(examples):\n",
    "        cell_text.append([f\"Example {i+1}\", gt, gen])\n",
    "\n",
    "    table = ax5.table(cellText=cell_text,\n",
    "                     colLabels=[\"\", \"Ground Truth\", \"Generated Caption\"],\n",
    "                     cellLoc='left',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.1, 0.45, 0.45])\n",
    "\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "\n",
    "    \n",
    "    for j in range(3):\n",
    "        table[(0, j)].set_facecolor('\n",
    "        table[(0, j)].set_text_props(color='white', fontweight='bold')\n",
    "\n",
    "    \n",
    "    for i in range(len(examples)):\n",
    "        row = i + 1\n",
    "        color = '\n",
    "        for j in range(3):\n",
    "            table[(row, j)].set_facecolor(color)\n",
    "\n",
    "    ax5.set_title('Caption Examples', fontsize=14)\n",
    "\n",
    "    \n",
    "    plt.suptitle('NSVA (NBA Sports Video Analysis) Results Dashboard', fontsize=16, y=0.98)\n",
    "    plt.figtext(0.5, 0.01, f'Generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M\")}',\n",
    "               ha='center', fontsize=10, style='italic')\n",
    "\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(os.path.join(output_dir, 'results_dashboard.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    ANNOTATIONS_FILE = '/content/drive/MyDrive/NSVA_Results/annotations/annotations.json'\n",
    "    FEATURE_PATHS = {\n",
    "        'timesformer': '/content/drive/MyDrive/NSVA_Results/features/timesformer',\n",
    "        'ball': '/content/drive/MyDrive/NSVA_Results/features/ball',\n",
    "        'player': '/content/drive/MyDrive/NSVA_Results/features/player',\n",
    "        'basket': '/content/drive/MyDrive/NSVA_Results/features/basket',\n",
    "        'court': '/content/drive/MyDrive/NSVA_Results/features/court',\n",
    "    }\n",
    "\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NSVA_Results/results'\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/NSVA_Results/checkpoints'\n",
    "\n",
    "    MAX_SEQ_LENGTH = 30\n",
    "    BATCH_SIZE = 32  \n",
    "    EMBED_DIM = 256\n",
    "    NUM_HEADS = 4\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4  \n",
    "\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    \n",
    "    dataset = NSVADataset(\n",
    "        ANNOTATIONS_FILE,\n",
    "        FEATURE_PATHS,\n",
    "        tokenizer,\n",
    "        MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    all_video_ids = dataset.available_videos\n",
    "    game_ids = set()\n",
    "    video_to_game = {}\n",
    "\n",
    "    for video_id in all_video_ids:\n",
    "        \n",
    "        game_id = video_id.split('_')[0]\n",
    "        game_ids.add(game_id)\n",
    "        video_to_game[video_id] = game_id\n",
    "\n",
    "    \n",
    "    game_ids = list(game_ids)\n",
    "    random.shuffle(game_ids)\n",
    "\n",
    "    train_games = game_ids[:int(0.8 * len(game_ids))]\n",
    "    val_games = game_ids[int(0.8 * len(game_ids)):int(0.9 * len(game_ids))]\n",
    "    test_games = game_ids[int(0.9 * len(game_ids)):]\n",
    "\n",
    "    \n",
    "    train_ids = [vid for vid in all_video_ids if video_to_game[vid] in train_games]\n",
    "    val_ids = [vid for vid in all_video_ids if video_to_game[vid] in val_games]\n",
    "    test_ids = [vid for vid in all_video_ids if video_to_game[vid] in test_games]\n",
    "\n",
    "    \n",
    "    train_dataset = dataset.create_tf_dataset(BATCH_SIZE, shuffle=True)\n",
    "    val_dataset = dataset.create_tf_dataset(BATCH_SIZE, shuffle=False)\n",
    "    test_dataset = dataset.create_tf_dataset(BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = NSVAModel(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_caption_length=MAX_SEQ_LENGTH,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS\n",
    "    )\n",
    "\n",
    "    \n",
    "    for inputs, targets in train_dataset.take(1):\n",
    "        _ = model(inputs, training=False)\n",
    "        break\n",
    "\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    history = train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        patience=5,\n",
    "        checkpoint_path=f'{CHECKPOINT_DIR}/nsva_model'\n",
    "    )\n",
    "\n",
    "    \n",
    "    with open(f'{OUTPUT_DIR}/training_history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    \n",
    "    visualize_training_history(history, OUTPUT_DIR)\n",
    "\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        model,\n",
    "        test_dataset,\n",
    "        tokenizer,\n",
    "        num_samples=100,  \n",
    "        output_path=f'{OUTPUT_DIR}/evaluation_results.json'\n",
    "    )\n",
    "\n",
    "    \n",
    "    visualize_evaluation_metrics(results, OUTPUT_DIR)\n",
    "    visualize_caption_examples(results.get('examples', []), OUTPUT_DIR)\n",
    "    visualize_feature_importance(OUTPUT_DIR)\n",
    "\n",
    "    \n",
    "    create_results_dashboard(history, results, results.get('examples', []), OUTPUT_DIR)\n",
    "\n",
    "    \n",
    "    model.save_weights(f'{CHECKPOINT_DIR}/nsva_model_final.weights.h5')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
