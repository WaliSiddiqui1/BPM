{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMHMEBEdFdvSwO/Sll3LkZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaliSiddiqui1/BPM/blob/main/Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dtbY-C6pikj1",
        "outputId": "ae980302-76e5-4102-88b5-acf3110f0ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.24.7 which is incompatible.\n",
            "transformers 4.51.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 0.24.7 which is incompatible.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.24.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yolov5 7.0.14 requires huggingface-hub<0.25.0,>=0.12.0, but you have huggingface-hub 0.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mMounting Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory structure setup complete.\n",
            "\n",
            "Checking video directory:\n",
            "Found 213 video files in /content/drive/MyDrive/NSVA_Results/raw_videos\n",
            "Sample files: ['Boston Celtics vs LA Clippers Full Game Highlights ｜ March 11, 2018-19 NBA Season.mp4', 'Atlanta Hawks vs Memphis Grizzlies Full Game Highlights ｜ March 13, 2018-19 NBA Season.mkv', 'Cleveland Cavaliers vs Philadelphia Sixers Full Game Highlights ｜ March 12, 2018-19 NBA Season.mkv']\n",
            "\n",
            "Checking caption directory:\n",
            "Found 215 VTT files in /content/drive/MyDrive/NSVA_Results/video_captions\n",
            "Sample files: ['Milwaukee Bucks vs New Orleans Pelicans Full Game Highlights ｜ March 12, 2018-19 NBA Season.en.vtt', 'Sacramento Kings vs Washington Wizards Full Game Highlights ｜ March 11, 2018-19 NBA Season.en.vtt', 'LA Clippers vs Indiana Pacers Full Game Highlights ｜ March 19, 2018-19 NBA Season.en.vtt']\n",
            "\n",
            "Found 2 unmatched VTT files. Attempting fuzzy matching...\n",
            "Matched 'Sacramento Kings vs Washington Wizards Full Game Highlights ｜ March 11, 2018-19 NBA Season.en.vtt' to 'Orlando Magic vs Washington Wizards Full Game Highlights ｜ March 13, 2018-19 NBA Season.mkv'\n",
            "Matched 'GS Warriors vs Toronto Raptors - Epic Game, OT ｜ Full Game Highlights ｜ 11.29.2018.en.vtt' to 'Toronto Raptors vs Orlando Magic Full Game Highlights ｜ April 1, 2018-19 NBA Season.mkv'\n",
            "\n",
            "Successfully mapped 215 of 215 VTT files to videos\n",
            "\n",
            "Sample mappings (VTT -> Video):\n",
            "1. Milwaukee Bucks vs New Orleans Pelicans Full Game Highlights ｜ March 12, 2018-19 NBA Season.en.vtt -> Milwaukee Bucks vs New Orleans Pelicans Full Game Highlights ｜ March 12, 2018-19 NBA Season.mkv\n",
            "2. LA Clippers vs Indiana Pacers Full Game Highlights ｜ March 19, 2018-19 NBA Season.en.vtt -> LA Clippers vs Indiana Pacers Full Game Highlights ｜ March 19, 2018-19 NBA Season.mkv\n",
            "3. Denver Nuggets vs Minnesota Timberwolves Full Game Highlights ｜ March 12, 2018-19 NBA Season.en.vtt -> Denver Nuggets vs Minnesota Timberwolves Full Game Highlights ｜ March 12, 2018-19 NBA Season.mp4\n",
            "4. New York Knicks vs Indiana Pacers Full Game Highlights ｜ March 12, 2018-19 NBA Season.en.vtt -> New York Knicks vs Indiana Pacers Full Game Highlights ｜ March 12, 2018-19 NBA Season.mkv\n",
            "5. LA Clippers vs Portland Trail Blazers Full Game Highlights ｜ March 12, 2018-19 NBA Season.en.vtt -> LA Clippers vs Portland Trail Blazers Full Game Highlights ｜ March 12, 2018-19 NBA Season.mkv\n",
            "Train: 108/146 processed (74.0%)\n",
            "Validation: 23/32 processed (71.9%)\n",
            "Test: 30/35 processed (85.7%)\n",
            "Total: 161/213 processed (75.6%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import re\n",
        "import time\n",
        "\n",
        "!pip install -q webvtt-py\n",
        "!pip install -q transformers\n",
        "!pip install -q rouge\n",
        "!pip install -q nltk\n",
        "!pip install -q yolov5\n",
        "\n",
        "!pip install -q huggingface-hub>=0.30.0 --upgrade\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import webvtt\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/NSVA_Results/'\n",
        "RAW_VIDEOS_DIR = os.path.join(BASE_DIR, 'raw_videos')\n",
        "VTT_CAPTIONS_DIR = os.path.join(BASE_DIR, 'video_captions')\n",
        "FEATURES_DIR = os.path.join(BASE_DIR, 'features')\n",
        "ANNOTATIONS_DIR = os.path.join(BASE_DIR, 'annotations')\n",
        "CHECKPOINTS_DIR = os.path.join(BASE_DIR, 'checkpoints')\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, 'results')\n",
        "\n",
        "for directory in [RAW_VIDEOS_DIR, VTT_CAPTIONS_DIR, FEATURES_DIR, ANNOTATIONS_DIR,\n",
        "                 CHECKPOINTS_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "for feature_type in ['timesformer', 'ball', 'player', 'basket', 'court']:\n",
        "    os.makedirs(os.path.join(FEATURES_DIR, feature_type), exist_ok=True)\n",
        "\n",
        "METADATA_DIR = os.path.join(BASE_DIR, 'metadata')\n",
        "os.makedirs(METADATA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\nChecking video directory:\")\n",
        "if os.path.exists(RAW_VIDEOS_DIR):\n",
        "    video_files = [f for f in os.listdir(RAW_VIDEOS_DIR) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
        "    print(f\"Found {len(video_files)} video files in {RAW_VIDEOS_DIR}\")\n",
        "    if len(video_files) > 0:\n",
        "        print(f\"Sample files: {video_files[:3]}\")\n",
        "\n",
        "print(\"\\nChecking caption directory:\")\n",
        "if os.path.exists(VTT_CAPTIONS_DIR):\n",
        "    vtt_files = [f for f in os.listdir(VTT_CAPTIONS_DIR) if f.endswith('.en.vtt')]\n",
        "    print(f\"Found {len(vtt_files)} VTT files in {VTT_CAPTIONS_DIR}\")\n",
        "    if len(vtt_files) > 0:\n",
        "        print(f\"Sample files: {vtt_files[:3]}\")\n",
        "\n",
        "# ===== 1. Create file mapping between VTT and video files =====\n",
        "def create_file_mapping():\n",
        "    \"\"\"Create a mapping between VTT caption files and video files\"\"\"\n",
        "    vtt_files = [f for f in os.listdir(VTT_CAPTIONS_DIR) if f.endswith('.en.vtt')]\n",
        "    video_files = [f for f in os.listdir(RAW_VIDEOS_DIR) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
        "\n",
        "    mapping = {}\n",
        "\n",
        "    for vtt_file in vtt_files:\n",
        "        vtt_base = vtt_file[:-7]\n",
        "\n",
        "        for video_file in video_files:\n",
        "            video_base = os.path.splitext(video_file)[0]\n",
        "\n",
        "            if vtt_base == video_base:\n",
        "                mapping[vtt_file] = video_file\n",
        "                break\n",
        "\n",
        "    unmatched_vtts = [vtt for vtt in vtt_files if vtt not in mapping]\n",
        "    if unmatched_vtts:\n",
        "        print(f\"\\nFound {len(unmatched_vtts)} unmatched VTT files. Attempting fuzzy matching...\")\n",
        "\n",
        "        for vtt_file in unmatched_vtts:\n",
        "            vtt_base = vtt_file[:-7]\n",
        "\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            for video_file in video_files:\n",
        "                video_base = os.path.splitext(video_file)[0]\n",
        "\n",
        "                common_chars = sum(1 for c in vtt_base if c in video_base)\n",
        "                score = common_chars / max(len(vtt_base), len(video_base))\n",
        "\n",
        "                if score > best_score and score > 0.7:\n",
        "                    best_match = video_file\n",
        "                    best_score = score\n",
        "\n",
        "            if best_match:\n",
        "                mapping[vtt_file] = best_match\n",
        "                print(f\"Matched '{vtt_file}' to '{best_match}'\")\n",
        "\n",
        "    print(f\"\\nSuccessfully mapped {len(mapping)} of {len(vtt_files)} VTT files to videos\")\n",
        "\n",
        "    with open(os.path.join(METADATA_DIR, 'file_mapping.txt'), 'w') as f:\n",
        "        for vtt, video in mapping.items():\n",
        "            f.write(f\"{vtt}|{video}\\n\")\n",
        "\n",
        "    return mapping\n",
        "\n",
        "file_mapping = create_file_mapping()\n",
        "\n",
        "print(\"\\nSample mappings (VTT -> Video):\")\n",
        "for i, (vtt, video) in enumerate(list(file_mapping.items())[:5]):\n",
        "    print(f\"{i+1}. {vtt} -> {video}\")\n",
        "\n",
        "# ===== 2. VTT Caption Processing =====\n",
        "def process_vtt_files(file_mapping):\n",
        "    \"\"\"\n",
        "    Process VTT caption files into structured annotations\n",
        "    \"\"\"\n",
        "    print(\"Processing VTT caption files...\")\n",
        "\n",
        "    annotations = {'sentences': []}\n",
        "    video_captions = {}\n",
        "\n",
        "    for vtt_file, video_file in tqdm(file_mapping.items()):\n",
        "        video_id = os.path.splitext(video_file)[0]\n",
        "        vtt_path = os.path.join(VTT_CAPTIONS_DIR, vtt_file)\n",
        "\n",
        "        try:\n",
        "            captions = webvtt.read(vtt_path)\n",
        "\n",
        "            video_captions[video_id] = []\n",
        "\n",
        "            for caption in captions:\n",
        "                start_time = caption.start\n",
        "                end_time = caption.end\n",
        "                text = caption.text.strip()\n",
        "\n",
        "                start_seconds = convert_time_to_seconds(start_time)\n",
        "                end_seconds = convert_time_to_seconds(end_time)\n",
        "\n",
        "                caption_entry = {\n",
        "                    'video_id': video_id,\n",
        "                    'start_time': start_time,\n",
        "                    'end_time': end_time,\n",
        "                    'start_seconds': start_seconds,\n",
        "                    'end_seconds': end_seconds,\n",
        "                    'caption': text\n",
        "                }\n",
        "\n",
        "                video_captions[video_id].append(caption_entry)\n",
        "\n",
        "                annotations['sentences'].append({\n",
        "                    'video_id': video_id,\n",
        "                    'caption': text,\n",
        "                    'start': start_seconds,\n",
        "                    'end': end_seconds\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {vtt_file}: {str(e)}\")\n",
        "\n",
        "    annotations_file = os.path.join(ANNOTATIONS_DIR, 'annotations.json')\n",
        "    with open(annotations_file, 'w') as f:\n",
        "        json.dump(annotations, f, indent=2)\n",
        "\n",
        "    for video_id, captions in video_captions.items():\n",
        "        video_captions_file = os.path.join(ANNOTATIONS_DIR, f'{video_id}_captions.json')\n",
        "        with open(video_captions_file, 'w') as f:\n",
        "            json.dump(captions, f, indent=2)\n",
        "\n",
        "    print(f\"Processed {len(file_mapping)} VTT files with {len(annotations['sentences'])} total captions.\")\n",
        "    return annotations\n",
        "\n",
        "def convert_time_to_seconds(time_str):\n",
        "    \"\"\"Convert VTT time format to seconds\"\"\"\n",
        "    h, m, s = time_str.split(':')\n",
        "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
        "\n",
        "# ===== 3. Dataset Splitting =====\n",
        "video_files = [f for f in os.listdir(RAW_VIDEOS_DIR) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
        "\n",
        "def create_dataset_splits(annotations, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, min_test=35):\n",
        "    \"\"\"\n",
        "    Create train/val/test splits with appropriate distribution of teams and actions\n",
        "    \"\"\"\n",
        "    print(\"Creating dataset splits...\")\n",
        "\n",
        "    video_ids = list(set([s['video_id'] for s in annotations['sentences']]))\n",
        "    print(f\"Found {len(video_ids)} unique videos\")\n",
        "\n",
        "    video_metadata = {}\n",
        "\n",
        "    for video_id in video_ids:\n",
        "        video_captions = [s for s in annotations['sentences'] if s['video_id'] == video_id]\n",
        "\n",
        "        team_mentions = []\n",
        "        for caption in video_captions:\n",
        "            team_pattern = r'\\b(Bucks|Lakers|Celtics|Warriors|Heat|Spurs|Bulls|Rockets)\\b'\n",
        "            teams_found = re.findall(team_pattern, caption['caption'])\n",
        "            team_mentions.extend(teams_found)\n",
        "\n",
        "        actions = {\n",
        "            'shot': sum(1 for c in video_captions if 'shot' in c['caption'].lower()),\n",
        "            'rebound': sum(1 for c in video_captions if 'rebound' in c['caption'].lower()),\n",
        "            'assist': sum(1 for c in video_captions if 'assist' in c['caption'].lower()),\n",
        "            'block': sum(1 for c in video_captions if 'block' in c['caption'].lower()),\n",
        "            'steal': sum(1 for c in video_captions if 'steal' in c['caption'].lower())\n",
        "        }\n",
        "\n",
        "        video_metadata[video_id] = {\n",
        "            'teams': list(set(team_mentions)),\n",
        "            'actions': actions,\n",
        "            'caption_count': len(video_captions)\n",
        "        }\n",
        "\n",
        "    import random\n",
        "    random.seed(42)\n",
        "\n",
        "    shuffled_videos = video_ids.copy()\n",
        "    random.shuffle(shuffled_videos)\n",
        "\n",
        "    n_videos = len(shuffled_videos)\n",
        "    test_size = max(int(n_videos * test_ratio), min_test)\n",
        "\n",
        "    remaining = n_videos - test_size\n",
        "\n",
        "    n_train = int(remaining * (train_ratio / (train_ratio + val_ratio)))\n",
        "    n_val = remaining - n_train\n",
        "\n",
        "    train_videos = shuffled_videos[:n_train]\n",
        "    val_videos = shuffled_videos[n_train:n_train+n_val]\n",
        "    test_videos = shuffled_videos[n_train+n_val:]\n",
        "\n",
        "    splits = {\n",
        "        'train': train_videos,\n",
        "        'val': val_videos,\n",
        "        'test': test_videos\n",
        "    }\n",
        "\n",
        "    splits_file = os.path.join(METADATA_DIR, 'splits.json')\n",
        "    with open(splits_file, 'w') as f:\n",
        "        json.dump(splits, f, indent=2)\n",
        "\n",
        "    print(f\"Created dataset splits: {len(train_videos)} train, {len(val_videos)} val, {len(test_videos)} test\")\n",
        "    return splits\n",
        "\n",
        "# ==========================================\n",
        "# Initial extraction didn't go as planned, had to pick processing back up\n",
        "\n",
        "def resume_processing(file_mapping, splits):\n",
        "    \"\"\"Resume processing from where we left off, with specific quotas for each split\"\"\"\n",
        "    processed_videos = set()\n",
        "    state_file = os.path.join(METADATA_DIR, 'processed_videos.txt')\n",
        "    if os.path.exists(state_file):\n",
        "        with open(state_file, 'r') as f:\n",
        "            processed_videos = set(f.read().splitlines())\n",
        "\n",
        "    unprocessed = {\n",
        "        'train': [v for v in splits['train'] if v not in processed_videos],\n",
        "        'val': [v for v in splits['val'] if v not in processed_videos],\n",
        "        'test': [v for v in splits['test'] if v not in processed_videos]\n",
        "    }\n",
        "\n",
        "    quotas = {\n",
        "        'train': min(5, len(unprocessed['train'])),\n",
        "        'val': min(5, len(unprocessed['val'])),\n",
        "        'test': min(30, len(unprocessed['test']))\n",
        "    }\n",
        "\n",
        "    print(f\"Unprocessed videos: {len(unprocessed['train'])} train, \"\n",
        "          f\"{len(unprocessed['val'])} val, {len(unprocessed['test'])} test\")\n",
        "    print(f\"Will process: {quotas['train']} train, {quotas['val']} val, \"\n",
        "          f\"{quotas['test']} test videos\")\n",
        "\n",
        "    to_process = {\n",
        "        'train': unprocessed['train'][:quotas['train']],\n",
        "        'val': unprocessed['val'][:quotas['val']],\n",
        "        'test': unprocessed['test'][:quotas['test']]\n",
        "    }\n",
        "\n",
        "    return to_process, processed_videos\n",
        "\n",
        "# ===== 4. Feature Extraction Pipeline =====\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "\n",
        "def load_yolo_model():\n",
        "    \"\"\"Load YOLOv5 model for object detection\"\"\"\n",
        "    print(\"Loading YOLOv5 model...\")\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "    # Set classes to detect people (class 0) and sports ball (class 32)\n",
        "    model.classes = [0, 32]\n",
        "    return model\n",
        "\n",
        "def load_vit_model():\n",
        "    \"\"\"Load Vision Transformer model for feature extraction\"\"\"\n",
        "    print(\"Loading ViT model...\")\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "    model = ViTModel.from_pretrained('google/vit-base-patch16-224', add_pooling_layer=False)\n",
        "    return feature_extractor, model\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, yolo_model, vit_extractor, vit_model, file_mapping):\n",
        "        self.yolo = yolo_model\n",
        "        self.vit_extractor = vit_extractor\n",
        "        self.vit_model = vit_model\n",
        "        self.file_mapping = file_mapping\n",
        "\n",
        "        self.video_id_to_file = {}\n",
        "        for vtt_file, video_file in file_mapping.items():\n",
        "            video_id = os.path.splitext(video_file)[0]\n",
        "            self.video_id_to_file[video_id] = video_file\n",
        "\n",
        "        self.processed_videos = set()\n",
        "        state_file = os.path.join(METADATA_DIR, 'processed_videos.txt')\n",
        "        if os.path.exists(state_file):\n",
        "            with open(state_file, 'r') as f:\n",
        "                self.processed_videos = set(f.read().splitlines())\n",
        "\n",
        "    def extract_frames(self, video_path, sample_rate=8):\n",
        "        \"\"\"Extract frames from video at given sample rate (fps)\"\"\"\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        interval = max(1, int(fps / sample_rate))\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % interval == 0:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(frame)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def detect_objects(self, frame):\n",
        "        \"\"\"Detect ball, players and basket using YOLOv5\"\"\"\n",
        "        pil_image = Image.fromarray(frame)\n",
        "        results = self.yolo(pil_image)\n",
        "        return results\n",
        "\n",
        "    def extract_ball_features(self, frame, detections):\n",
        "        \"\"\"Extract ball features using ViT\"\"\"\n",
        "        ball_detections = detections.xyxy[0][detections.xyxy[0][:, 5] == 32]\n",
        "\n",
        "        if len(ball_detections) == 0:\n",
        "            return np.zeros(768)\n",
        "\n",
        "        best_ball = ball_detections[torch.argmax(ball_detections[:, 4])]\n",
        "        x1, y1, x2, y2 = best_ball[:4].int().cpu().numpy()\n",
        "\n",
        "        h, w = frame.shape[:2]\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "        if x1 >= x2 or y1 >= y2:\n",
        "            return np.zeros(768)\n",
        "\n",
        "        ball_crop = frame[y1:y2, x1:x2]\n",
        "\n",
        "        ball_crop_pil = Image.fromarray(ball_crop)\n",
        "        inputs = self.vit_extractor(images=ball_crop_pil, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vit_model(**inputs)\n",
        "\n",
        "        ball_features = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return ball_features\n",
        "\n",
        "    def extract_player_features(self, frame, detections):\n",
        "        \"\"\"Extract player features using ViT\"\"\"\n",
        "        player_detections = detections.xyxy[0][detections.xyxy[0][:, 5] == 0]\n",
        "\n",
        "        max_players = 5\n",
        "        player_features = np.zeros((max_players, 768))\n",
        "\n",
        "        for i, player in enumerate(player_detections[:max_players]):\n",
        "            x1, y1, x2, y2 = player[:4].int().cpu().numpy()\n",
        "\n",
        "            h, w = frame.shape[:2]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "            if x1 >= x2 or y1 >= y2:\n",
        "                continue\n",
        "\n",
        "            player_crop = frame[y1:y2, x1:x2]\n",
        "\n",
        "            player_crop_pil = Image.fromarray(player_crop)\n",
        "            inputs = self.vit_extractor(images=player_crop_pil, return_tensors=\"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.vit_model(**inputs)\n",
        "\n",
        "            player_features[i] = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return player_features\n",
        "\n",
        "    def extract_basket_features(self, frame, detections):\n",
        "        \"\"\"\n",
        "        Extract basket features using ViT\n",
        "        Note: YOLOv5 base model doesn't have a basketball hoop class\n",
        "        This is a simplified approach - ideally we'd use a custom trained model\n",
        "        \"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "        upper_frame = frame[:h//3, :]\n",
        "\n",
        "        basket_crop_pil = Image.fromarray(upper_frame)\n",
        "        inputs = self.vit_extractor(images=basket_crop_pil, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vit_model(**inputs)\n",
        "\n",
        "        basket_features = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return basket_features\n",
        "\n",
        "    def generate_court_segmentation(self, frame):\n",
        "        \"\"\"\n",
        "        Generate court line segmentation\n",
        "        This is a simplified placeholder - in real implementation,\n",
        "        we'd use a court line segmentation model\n",
        "        \"\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "        edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "        edges_pil = Image.fromarray(edges_rgb)\n",
        "\n",
        "        inputs = self.vit_extractor(images=edges_pil, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vit_model(**inputs)\n",
        "\n",
        "        court_features = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return court_features\n",
        "\n",
        "    def extract_features(self, video_id, max_frames=100):\n",
        "        \"\"\"Extract all features for a video\"\"\"\n",
        "        if video_id in self.processed_videos:\n",
        "            print(f\"Video {video_id} already processed. Skipping.\")\n",
        "            return True\n",
        "\n",
        "        print(f\"Processing video {video_id}...\")\n",
        "\n",
        "        if video_id not in self.video_id_to_file:\n",
        "            print(f\"Video ID {video_id} not found in file mapping\")\n",
        "            return False\n",
        "\n",
        "        video_file = self.video_id_to_file[video_id]\n",
        "        video_path = os.path.join(RAW_VIDEOS_DIR, video_file)\n",
        "\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"Video file not found: {video_path}\")\n",
        "            return False\n",
        "\n",
        "        frames = self.extract_frames(video_path)\n",
        "        if len(frames) == 0:\n",
        "            print(f\"Failed to extract frames from {video_path}\")\n",
        "            return False\n",
        "\n",
        "        frames = frames[:max_frames]\n",
        "        n_frames = len(frames)\n",
        "\n",
        "        timesformer_features = np.zeros((n_frames, 768))\n",
        "        ball_features = np.zeros((n_frames, 768))\n",
        "        player_features = np.zeros((n_frames, 5, 768))\n",
        "        basket_features = np.zeros((n_frames, 768))\n",
        "        court_features = np.zeros((n_frames, 768))\n",
        "\n",
        "        for i, frame in enumerate(tqdm(frames, desc=f\"Processing frames for {video_id}\")):\n",
        "            detections = self.detect_objects(frame)\n",
        "\n",
        "            ball_features[i] = self.extract_ball_features(frame, detections)\n",
        "            player_features[i] = self.extract_player_features(frame, detections)\n",
        "            basket_features[i] = self.extract_basket_features(frame, detections)\n",
        "            court_features[i] = self.generate_court_segmentation(frame)\n",
        "\n",
        "            timesformer_features[i] = np.random.randn(768) * 0.1\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                np.save(os.path.join(FEATURES_DIR, 'ball', f\"{video_id}_temp.npy\"), ball_features[:i+1])\n",
        "                np.save(os.path.join(FEATURES_DIR, 'player', f\"{video_id}_temp.npy\"), player_features[:i+1])\n",
        "                np.save(os.path.join(FEATURES_DIR, 'basket', f\"{video_id}_temp.npy\"), basket_features[:i+1])\n",
        "                np.save(os.path.join(FEATURES_DIR, 'court', f\"{video_id}_temp.npy\"), court_features[:i+1])\n",
        "\n",
        "        np.save(os.path.join(FEATURES_DIR, 'timesformer', f\"{video_id}.npy\"), timesformer_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'ball', f\"{video_id}.npy\"), ball_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'player', f\"{video_id}.npy\"), player_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'basket', f\"{video_id}.npy\"), basket_features)\n",
        "        np.save(os.path.join(FEATURES_DIR, 'court', f\"{video_id}.npy\"), court_features)\n",
        "\n",
        "        for feature_type in ['ball', 'player', 'basket', 'court']:\n",
        "            temp_file = os.path.join(FEATURES_DIR, feature_type, f\"{video_id}_temp.npy\")\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "        self.processed_videos.add(video_id)\n",
        "        with open(os.path.join(METADATA_DIR, 'processed_videos.txt'), 'a') as f:\n",
        "            f.write(f\"{video_id}\\n\")\n",
        "\n",
        "        print(f\"Completed processing video {video_id}\")\n",
        "        return True\n",
        "\n",
        "# ===== 5. Create Captions CSV for Dataloader =====\n",
        "def create_captions_csv(annotations):\n",
        "    \"\"\"Create CSV file with captions for the dataloader\"\"\"\n",
        "    print(\"Creating captions CSV...\")\n",
        "\n",
        "    captions_data = []\n",
        "\n",
        "    for sentence in annotations['sentences']:\n",
        "        video_id = sentence['video_id']\n",
        "        caption = sentence['caption']\n",
        "\n",
        "        captions_data.append({\n",
        "            'video_id': video_id,\n",
        "            'caption': caption,\n",
        "            'feature_file': f\"{video_id}.npy\"\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(captions_data)\n",
        "    df.to_csv(os.path.join(METADATA_DIR, 'captions.csv'), index=False)\n",
        "    print(f\"Exported {len(captions_data)} captions to CSV\")\n",
        "\n",
        "# ===== 6. Main Pipeline Execution =====\n",
        "def main():\n",
        "    \"\"\"Execute the full feature extraction pipeline\"\"\"\n",
        "    file_mapping = create_file_mapping()\n",
        "\n",
        "    annotations = process_vtt_files(file_mapping)\n",
        "\n",
        "    splits = create_dataset_splits(annotations)\n",
        "\n",
        "    create_captions_csv(annotations)\n",
        "\n",
        "    to_process, processed_videos = resume_processing(file_mapping, splits)\n",
        "\n",
        "    yolo_model = load_yolo_model()\n",
        "    vit_extractor, vit_model = load_vit_model()\n",
        "\n",
        "    extractor = FeatureExtractor(yolo_model, vit_extractor, vit_model, file_mapping)\n",
        "    extractor.processed_videos = processed_videos\n",
        "\n",
        "    train_videos = to_process['train']\n",
        "    print(f\"Processing {len(train_videos)} training videos...\")\n",
        "\n",
        "    for i, video_id in enumerate(train_videos):\n",
        "        print(f\"Processing video {i+1}/{len(train_videos)}: {video_id}\")\n",
        "        success = extractor.extract_features(video_id)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(\"Clearing memory cache...\")\n",
        "            torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    val_videos = to_process['val']\n",
        "    print(f\"Processing {len(val_videos)} validation videos...\")\n",
        "\n",
        "    for i, video_id in enumerate(val_videos):\n",
        "        print(f\"Processing video {i+1}/{len(val_videos)}: {video_id}\")\n",
        "        success = extractor.extract_features(video_id)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(\"Clearing memory cache...\")\n",
        "            torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    test_videos = to_process['test']\n",
        "    print(f\"Processing {len(test_videos)} test videos...\")\n",
        "\n",
        "    for i, video_id in enumerate(test_videos):\n",
        "        print(f\"Processing video {i+1}/{len(test_videos)}: {video_id}\")\n",
        "        success = extractor.extract_features(video_id)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(\"Clearing memory cache...\")\n",
        "            torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    print(\"Feature extraction pipeline complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}